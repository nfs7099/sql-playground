# LLM (OpenAI-compatible local endpoint; e.g., Ollama)
LLM_HOST=http://localhost
LLM_PORT=11434
LLM_MODEL=gemma2:9b
LLM_API_BASE=${LLM_HOST}:${LLM_PORT}/v1
LLM_API_KEY=ollama  # dummy key for OpenAI-compatible clients

# Paths (override if you move files)
QUESTIONS_PATH=questions/questions.yaml
SOLUTIONS_PATH=solutions/solutions.yaml
